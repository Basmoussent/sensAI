# ü§ñ TASKS AI - T√¢ches IA & Machine Learning

Ce document liste toutes les t√¢ches li√©es √† l'IA, au ML, et aux prompts pour le projet sensAI.

## üéØ Objectif
Cr√©er un sensei IA qui utilise la m√©thode socratique pour enseigner la programmation de mani√®re p√©dagogique et intelligente.

---

## üìã SPRINT 1 - MVP Enrichi (Semaine 1)

### ‚úÖ JOUR 1-2: Setup API + Code Analyzer

#### 1.1 Setup API Mistral (au lieu de vLLM local)
**Pourquoi:** Pas besoin de 13GB de RAM, focus sur l'architecture IA

**T√¢ches:**
- [ ] Cr√©er compte Mistral AI et obtenir API key
- [ ] Setup configuration avec variables d'environnement
- [ ] Cr√©er `backend/services/llm_service.py` basique
- [ ] Tester connection et premiers appels API
- [ ] Documenter les param√®tres (temperature, top_p, max_tokens)

**Fichiers √† cr√©er:**
```
backend/
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îî‚îÄ‚îÄ llm_service.py          # Service principal LLM
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îî‚îÄ‚îÄ config.py               # Configuration (API keys)
‚îî‚îÄ‚îÄ .env.example                # Template variables d'env
```

**Code exemple:**
```python
# backend/services/llm_service.py
from mistralai.client import MistralClient
import os

class LLMService:
    def __init__(self):
        self.client = MistralClient(api_key=os.getenv("MISTRAL_API_KEY"))

    def generate_response(self, messages, temperature=0.7):
        response = self.client.chat(
            model="mistral-small-latest",
            messages=messages,
            temperature=temperature,
            max_tokens=2048
        )
        return response.choices[0].message.content
```

---

#### 1.2 Code Analyzer avec AST (Comp√©tence: Static Analysis)
**Objectif:** Analyser le code AVANT d'envoyer au LLM pour enrichir le contexte

**T√¢ches:**
- [ ] Cr√©er `backend/ai/code_analyzer.py`
- [ ] Impl√©menter analyse AST pour Python
- [ ] D√©tecter complexit√© cyclomatique
- [ ] Identifier code smells (fonctions trop longues, trop de params, etc.)
- [ ] Scanner pour probl√®mes de s√©curit√© (eval, exec, etc.)
- [ ] Extraire m√©tadonn√©es (fonctions, imports, classes)

**Fichiers √† cr√©er:**
```
backend/
‚îî‚îÄ‚îÄ ai/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ code_analyzer.py         # Analyse AST
    ‚îî‚îÄ‚îÄ complexity.py            # Calcul complexit√©
```

**Code exemple:**
```python
import ast
from typing import Dict, List

class CodeAnalyzer:
    def analyze_python(self, code: str) -> Dict:
        try:
            tree = ast.parse(code)
        except SyntaxError as e:
            return {'syntax_error': str(e)}

        return {
            'complexity': self._calculate_complexity(tree),
            'functions': self._extract_functions(tree),
            'code_smells': self._detect_smells(tree),
            'security_issues': self._security_scan(tree),
        }
```

**Tests:**
- [ ] Tester avec code simple
- [ ] Tester avec code complexe
- [ ] Tester avec code bugg√©
- [ ] Documenter les patterns d√©tect√©s

---

### ‚úÖ JOUR 2-3: RAG System + Prompt Orchestrator

#### 2.1 RAG (Retrieval Augmented Generation)
**Objectif:** R√©cup√©rer des exemples similaires pour enrichir les prompts

**T√¢ches:**
- [ ] Setup SentenceTransformers pour embeddings
- [ ] Setup ChromaDB (ou FAISS) pour vector store
- [ ] Cr√©er base d'exemples de code (bon/mauvais)
- [ ] Impl√©menter recherche s√©mantique
- [ ] Int√©grer dans le service LLM

**Fichiers √† cr√©er:**
```
backend/
‚îî‚îÄ‚îÄ ai/
    ‚îú‚îÄ‚îÄ rag_system.py            # RAG principal
    ‚îú‚îÄ‚îÄ embeddings.py            # Gestion embeddings
    ‚îî‚îÄ‚îÄ examples/
        ‚îú‚îÄ‚îÄ python_examples.json
        ‚îú‚îÄ‚îÄ javascript_examples.json
        ‚îî‚îÄ‚îÄ common_mistakes.json
```

**Code exemple:**
```python
from sentence_transformers import SentenceTransformer
import chromadb

class RAGSystem:
    def __init__(self):
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        self.db = chromadb.Client()
        self.collection = self.db.create_collection("code_examples")

    def add_example(self, code: str, explanation: str, tags: List[str]):
        embedding = self.embedder.encode(code)
        self.collection.add(
            embeddings=[embedding.tolist()],
            documents=[code],
            metadatas=[{"explanation": explanation, "tags": tags}]
        )

    def retrieve_similar(self, code: str, top_k: int = 3):
        embedding = self.embedder.encode(code)
        results = self.collection.query(
            query_embeddings=[embedding.tolist()],
            n_results=top_k
        )
        return results
```

---

#### 2.2 Prompt Orchestrator
**Objectif:** Composer intelligemment les prompts selon le contexte

**T√¢ches:**
- [ ] Utiliser les prompts existants de `backend/prompts/`
- [ ] Cr√©er orchestrateur qui s√©lectionne les bons prompts
- [ ] Int√©grer r√©sultats du Code Analyzer
- [ ] Int√©grer exemples du RAG
- [ ] Adapter selon niveau d√©tect√© de l'√©tudiant

**Fichiers √† cr√©er:**
```
backend/
‚îî‚îÄ‚îÄ services/
    ‚îî‚îÄ‚îÄ prompt_orchestrator.py   # Orchestration intelligente
```

**Code exemple:**
```python
from backend.prompts import (
    SYSTEM_PROMPT, PYTHON_PROMPT,
    BEGINNER_FRIENDLY_PROMPT, ADVANCED_DEVELOPER_PROMPT
)

class PromptOrchestrator:
    def __init__(self, code_analyzer, rag_system):
        self.analyzer = code_analyzer
        self.rag = rag_system

    def build_review_prompt(self, code, language, student_level="intermediate"):
        # 1. Analyser le code
        analysis = self.analyzer.analyze_python(code)

        # 2. R√©cup√©rer exemples similaires
        examples = self.rag.retrieve_similar(code)

        # 3. S√©lectionner les prompts appropri√©s
        system = SYSTEM_PROMPT
        if student_level == "beginner":
            system += "\n\n" + BEGINNER_FRIENDLY_PROMPT

        system += "\n\n" + PYTHON_PROMPT

        # 4. Composer le prompt enrichi
        user_prompt = f"""Code √† reviewer:
```python
{code}
```

Analyse automatique d√©tect√©e:
- Complexit√©: {analysis['complexity']}
- Code smells: {analysis['code_smells']}
- Issues de s√©curit√©: {analysis['security_issues']}

Exemples similaires trouv√©s: {len(examples)} cas

En tant que sensei, guide l'√©tudiant avec la m√©thode socratique."""

        return system, user_prompt
```

---

### ‚úÖ JOUR 3-4: Conversation Memory + Service Complet

#### 3.1 Conversation Memory
**Objectif:** M√©moriser le contexte de la conversation et la progression

**T√¢ches:**
- [ ] Cr√©er syst√®me de m√©moire court-terme (5 derniers messages)
- [ ] Cr√©er syst√®me de m√©moire long-terme (concepts expliqu√©s)
- [ ] Profiler l'√©tudiant (niveau, erreurs r√©currentes)
- [ ] D√©tecter patterns d'apprentissage

**Fichiers √† cr√©er:**
```
backend/
‚îî‚îÄ‚îÄ ai/
    ‚îú‚îÄ‚îÄ conversation_memory.py   # M√©moire conversationnelle
    ‚îî‚îÄ‚îÄ student_profile.py       # Profil √©tudiant
```

**Code exemple:**
```python
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class Message:
    role: str
    content: str
    timestamp: str

class ConversationMemory:
    def __init__(self):
        self.short_term: List[Message] = []  # 5 derniers
        self.long_term: Dict = {
            'explained_concepts': set(),
            'recurring_mistakes': [],
            'student_level': 'intermediate'
        }

    def add_message(self, role: str, content: str):
        msg = Message(role, content, datetime.now().isoformat())
        self.short_term.append(msg)

        # Garder seulement les 5 derniers
        if len(self.short_term) > 5:
            self.short_term.pop(0)

        # Extraire concepts et mettre √† jour long-term
        self._update_long_term_memory(msg)

    def build_context_summary(self) -> str:
        """R√©sum√© intelligent pour √©conomiser tokens"""
        return f"""Contexte de la conversation:
- Niveau d√©tect√©: {self.long_term['student_level']}
- Concepts d√©j√† expliqu√©s: {', '.join(self.long_term['explained_concepts'])}
- Erreurs r√©currentes: {len(self.long_term['recurring_mistakes'])}
"""
```

---

#### 3.2 LLM Service Complet avec Streaming
**Objectif:** Service final int√©grant tous les composants

**T√¢ches:**
- [ ] Int√©grer Code Analyzer, RAG, Memory, Orchestrator
- [ ] Impl√©menter streaming SSE (Server-Sent Events)
- [ ] Post-processing des r√©ponses (markdown, highlighting)
- [ ] Gestion d'erreurs robuste
- [ ] Logging et monitoring

**Fichiers √† cr√©er:**
```
backend/
‚îî‚îÄ‚îÄ services/
    ‚îî‚îÄ‚îÄ llm_service.py           # Service complet
```

**Code exemple:**
```python
from typing import AsyncGenerator
from mistralai.models.chat_completion import ChatMessage

class EnhancedLLMService:
    def __init__(self):
        self.client = MistralClient(api_key=os.getenv("MISTRAL_API_KEY"))
        self.analyzer = CodeAnalyzer()
        self.rag = RAGSystem()
        self.orchestrator = PromptOrchestrator(self.analyzer, self.rag)
        self.memory = ConversationMemory()

    async def review_code_stream(
        self,
        code: str,
        language: str,
        student_id: str
    ) -> AsyncGenerator[str, None]:
        # 1. Construire prompt enrichi
        system_prompt, user_prompt = self.orchestrator.build_review_prompt(
            code, language, self.memory.long_term['student_level']
        )

        # 2. Ajouter contexte conversationnel
        context = self.memory.build_context_summary()

        # 3. Composer messages
        messages = [
            ChatMessage(role="system", content=system_prompt),
            ChatMessage(role="user", content=context + "\n" + user_prompt)
        ]

        # 4. Streaming
        response = self.client.chat_stream(
            model="mistral-small-latest",
            messages=messages,
            temperature=0.7
        )

        full_response = ""
        for chunk in response:
            delta = chunk.choices[0].delta.content
            if delta:
                full_response += delta
                yield delta

        # 5. Mettre √† jour m√©moire
        self.memory.add_message("user", user_prompt)
        self.memory.add_message("assistant", full_response)
```

---

### ‚úÖ JOUR 4-5: √âvaluation + Tests + Documentation

#### 4.1 Framework d'√âvaluation de Prompts
**Objectif:** Mesurer et am√©liorer la qualit√© des r√©ponses

**T√¢ches:**
- [ ] Cr√©er dataset de test (codes + r√©ponses attendues)
- [ ] M√©triques de qualit√© (questions socratiques, pr√©cision, etc.)
- [ ] A/B testing de diff√©rents prompts
- [ ] Benchmarking des performances

**Fichiers √† cr√©er:**
```
backend/
‚îî‚îÄ‚îÄ ai/
    ‚îú‚îÄ‚îÄ prompt_evaluator.py      # √âvaluation
    ‚îî‚îÄ‚îÄ metrics.py               # M√©triques personnalis√©es
```

**Code exemple:**
```python
class PromptEvaluator:
    def __init__(self):
        self.test_cases = self._load_test_cases()

    def evaluate_prompt(self, prompt_template) -> Dict:
        scores = []
        for test_case in self.test_cases:
            response = self.llm.generate(prompt_template.format(**test_case))

            score = {
                'has_questions': self._count_questions(response),
                'no_direct_solution': not self._contains_solution(response),
                'identifies_issue': self._checks_issue_found(response),
                'socratic_score': self._socratic_score(response)
            }
            scores.append(score)

        return self._aggregate_metrics(scores)
```

---

#### 4.2 Tests Unitaires
**T√¢ches:**
- [ ] Tests pour Code Analyzer
- [ ] Tests pour RAG System
- [ ] Tests pour Conversation Memory
- [ ] Tests d'int√©gration du service complet

**Fichiers √† cr√©er:**
```
tests/
‚îú‚îÄ‚îÄ test_code_analyzer.py
‚îú‚îÄ‚îÄ test_rag_system.py
‚îú‚îÄ‚îÄ test_conversation_memory.py
‚îú‚îÄ‚îÄ test_prompt_orchestrator.py
‚îî‚îÄ‚îÄ test_llm_service.py
```

---

#### 4.3 Documentation Technique
**T√¢ches:**
- [ ] README pour backend/ai/
- [ ] Documentation des prompts
- [ ] Guide d'utilisation du service
- [ ] Benchmarks et m√©triques
- [ ] Exemples d'utilisation

---

## üì¶ LIVRABLES SPRINT 1

√Ä la fin de la semaine 1, tu auras:

‚úÖ **Code Analyzer** avec AST (analyse statique)
‚úÖ **RAG System** avec embeddings et vector search
‚úÖ **Conversation Memory** avec profiling √©tudiant
‚úÖ **Prompt Orchestrator** intelligent
‚úÖ **LLM Service** complet avec streaming
‚úÖ **Framework d'√©valuation** avec m√©triques
‚úÖ **Tests unitaires** > 70% coverage
‚úÖ **Documentation** compl√®te

---

## üìã BACKLOG - Priorit√© Moyenne (Sprint 2+)

### 2. ‚úÖ Am√©lioration Continue des Prompts
**Objectif:** It√©rer sur les prompts bas√©s sur les m√©triques

**T√¢ches:**
- [ ] A/B testing syst√©matique
- [ ] Fine-tuning des temp√©ratures par type de question
- [ ] Optimisation bas√©e sur feedback utilisateur
- [ ] Few-shot learning dynamique

---

### 2. üß† Impl√©mentation du Contexte Conversationnel Intelligent
**Objectif:** Le sensei doit se souvenir du contexte et progresser p√©dagogiquement

**T√¢ches:**
- [ ] Cr√©er un syst√®me de m√©moire conversationnelle
  - Tracking des concepts d√©j√† expliqu√©s
  - D√©tection des patterns d'erreurs r√©p√©t√©es
  - M√©morisation du niveau de compr√©hension du student
- [ ] Impl√©mentation d'un r√©sum√© intelligent de conversation
  - Condensation des √©changes pr√©c√©dents pour √©conomiser tokens
  - Extraction des points cl√©s p√©dagogiques
- [ ] Syst√®me de questions de suivi intelligentes
  - G√©n√©rer des questions bas√©es sur les r√©ponses pr√©c√©dentes
  - Guider progressivement vers la solution
- [ ] D√©tection de frustration ou blocage
  - Adapter le style p√©dagogique si le student est bloqu√©
  - Proposer des indices plus directs si n√©cessaire

**Fichiers concern√©s:**
- Nouveau: `backend/ai/conversation_memory.py`
- Nouveau: `backend/ai/context_manager.py`
- `backend/prompts.py`

---

### 3. üîç Analyse de Code Avanc√©e avec IA
**Objectif:** Enrichir l'analyse avec d√©tection automatique de patterns et probl√®mes

**T√¢ches:**
- [ ] D√©tection automatique du niveau de code (complexit√©)
  - Analyse syntaxique basique
  - Identification de patterns avanc√©s (design patterns, SOLID)
- [ ] Analyse de la qualit√© du code
  - D√©tection de code smells
  - Suggestions de refactoring
  - √âvaluation de la lisibilit√©
- [ ] D√©tection de bugs courants
  - Erreurs logiques fr√©quentes
  - Edge cases non g√©r√©s
  - Memory leaks ou probl√®mes de performance
- [ ] G√©n√©ration de tests sugg√©r√©s
  - Identifier les cas de test manquants
  - Proposer des sc√©narios de test

**Fichiers concern√©s:**
- Nouveau: `backend/ai/code_analyzer.py`
- Nouveau: `backend/ai/pattern_detector.py`

---

### 4. üéì Syst√®me de Progression P√©dagogique
**Objectif:** Suivre la progression de l'√©tudiant et adapter l'enseignement

**T√¢ches:**
- [ ] Cr√©er un mod√®le de comp√©tences par langage
  - Syntaxe de base
  - Structures de contr√¥le
  - POO / Functional Programming
  - Patterns avanc√©s
  - Performance et optimisation
- [ ] Syst√®me de tracking de progression
  - Quelles comp√©tences ont √©t√© travaill√©es
  - Quels concepts ont √©t√© compris
  - Quels points restent √† am√©liorer
- [ ] Recommandations intelligentes d'exercices
  - Sugg√©rer des challenges adapt√©s au niveau
  - Progression graduelle de difficult√©
- [ ] G√©n√©ration de rapports de progression
  - Points forts identifi√©s
  - Axes d'am√©lioration
  - Prochaines √©tapes sugg√©r√©es

**Fichiers concern√©s:**
- Nouveau: `backend/ai/skill_tracker.py`
- Nouveau: `backend/ai/recommendation_engine.py`
- Nouveau: `backend/models/student_progress.py`

---

## üìã BACKLOG - Priorit√© Moyenne

### 5. üéØ Fine-tuning du Mod√®le Mistral
**Objectif:** Adapter le mod√®le sp√©cifiquement pour l'enseignement du code

**T√¢ches:**
- [ ] Cr√©er un dataset d'entra√Ænement
  - Collecter des exemples de code avec review p√©dagogique
  - Annoter avec niveau de difficult√©
  - Inclure des dialogues socratiques de qualit√©
- [ ] Pr√©parer l'infrastructure de fine-tuning
  - Setup environnement de training
  - Scripts de preprocessing des donn√©es
- [ ] Exp√©rimenter avec diff√©rents hyperparam√®tres
  - Learning rate
  - Nombre d'epochs
  - Batch size
- [ ] √âvaluation du mod√®le fine-tun√©
  - M√©triques de qualit√© p√©dagogique
  - Comparaison avec mod√®le de base
- [ ] D√©ploiement du mod√®le am√©lior√©

**Fichiers concern√©s:**
- Nouveau: `backend/ai/training/`
- Nouveau: `backend/ai/evaluation/`

---

### 6. üí¨ G√©n√©ration d'Exemples et Contre-exemples
**Objectif:** Le sensei g√©n√®re automatiquement des exemples pour illustrer

**T√¢ches:**
- [ ] G√©n√©ration d'exemples de code corrects
  - √Ä partir d'un concept expliqu√©
  - Diff√©rents niveaux de complexit√©
- [ ] G√©n√©ration de contre-exemples (anti-patterns)
  - Montrer ce qu'il ne faut pas faire
  - Expliquer pourquoi c'est probl√©matique
- [ ] G√©n√©ration de variations d'un m√™me code
  - Diff√©rentes approches pour r√©soudre le m√™me probl√®me
  - Comparaison des avantages/inconv√©nients
- [ ] Cr√©ation d'exercices interactifs
  - "Trouve l'erreur dans ce code"
  - "Am√©liore ce code"

**Fichiers concern√©s:**
- Nouveau: `backend/ai/code_generator.py`
- Nouveau: `backend/ai/example_generator.py`

---

### 7. üåç Support Multi-langues (Langue naturelle)
**Objectif:** Support fran√ßais/anglais pour les explications

**T√¢ches:**
- [ ] D√©tection automatique de la langue de l'utilisateur
- [ ] Adaptation des prompts syst√®me en fran√ßais
- [ ] G√©n√©ration de r√©ponses bilingues
- [ ] Support de l'alternance code-switching (franglais technique)

**Fichiers concern√©s:**
- `backend/prompts.py`
- Nouveau: `backend/ai/language_detector.py`

---

## üìã BACKLOG - Am√©liorations Futures

### 8. ü§ù Syst√®me de Comparaison de Code
**Objectif:** Comparer diff√©rentes solutions √† un m√™me probl√®me

**T√¢ches:**
- [ ] Analyse comparative de plusieurs solutions
- [ ] √âvaluation automatique (lisibilit√©, performance, maintenabilit√©)
- [ ] G√©n√©ration de recommandations contextuelles

---

### 9. üìä Analytics IA sur les Patterns d'Apprentissage
**Objectif:** Identifier les difficult√©s communes des √©tudiants

**T√¢ches:**
- [ ] Clustering des erreurs fr√©quentes par langage
- [ ] Identification de patterns d'apprentissage
- [ ] G√©n√©ration de contenus p√©dagogiques cibl√©s

---

### 10. üéÆ Gamification avec IA
**Objectif:** Rendre l'apprentissage plus engageant

**T√¢ches:**
- [ ] Syst√®me de d√©fis g√©n√©r√©s par IA
- [ ] Adaptation dynamique de la difficult√©
- [ ] Recommandations personnalis√©es de parcours

---

## üìù Notes Techniques

### Configuration Mistral-7B-Instruct-v0.3
- **Mod√®le actuel:** `mistralai/Mistral-7B-Instruct-v0.3`
- **Framework:** vLLM pour l'inf√©rence
- **Context window:** 8192 tokens
- **Temp√©rature recommand√©e:** 0.7-0.8 (balance cr√©ativit√©/pr√©cision)
- **Top-p:** 0.9
- **Max tokens:** 2048 pour les r√©ponses

### Best Practices Prompting
1. **Structure claire:** Instructions ‚Üí Contexte ‚Üí Question ‚Üí Format attendu
2. **Few-shot learning:** Inclure 2-3 exemples dans les prompts critiques
3. **Chain of thought:** Demander au mod√®le d'expliquer son raisonnement
4. **Constraints explicites:** D√©finir clairement le style p√©dagogique attendu

### M√©triques de Qualit√© √† Suivre
- **Pertinence p√©dagogique:** Les explications aident-elles vraiment ?
- **Qualit√© socratique:** Les questions guident-elles vers la solution ?
- **Adaptation au niveau:** Le ton est-il appropri√© ?
- **Compl√©tude:** Tous les aspects du code sont-ils couverts ?
- **Clart√©:** Les explications sont-elles compr√©hensibles ?

---

## üîó Liens et Ressources

- [Mistral AI Documentation](https://docs.mistral.ai/)
- [vLLM Documentation](https://docs.vllm.ai/)
- [LangChain Prompting Guide](https://python.langchain.com/docs/modules/model_io/prompts/)
- [Socratic Teaching Methods](https://en.wikipedia.org/wiki/Socratic_method)

---

**Derni√®re mise √† jour:** 27 Octobre 2025
